{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510994c2-81a2-4a46-8ccb-680000475ddf",
   "metadata": {},
   "source": [
    "### Fetching Data from Excel File (For testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d4175-902f-4a82-9b7d-05c76f805198",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_csv_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"CSV file successfully read.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found at <file_path>.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The CSV file is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: The CSV file is malformed.\")\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06e7f0-39bc-4b8f-98ad-998bdc36c33a",
   "metadata": {},
   "source": [
    "### Fetching Data from API Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed31ef5-43b7-404e-9986-00cb8aecc067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def throughAPI(api_url):\n",
    "    \"\"\"\n",
    "    Connect to the API and retrieve data through it.\n",
    "\n",
    "    Args:\n",
    "        string: Link of the API\n",
    "\n",
    "    Returns:\n",
    "        JSON: Data fetched from the API\n",
    "    \"\"\"\n",
    "\n",
    "    # ***API Credentials WARNING\n",
    "    username = \"<USERNAME>\"\n",
    "    password = \"<PASSWORD>\"\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Connecting to API.... and pulling Data\")\n",
    "        response = requests.get(api_url, auth=HTTPBasicAuth(username, password), timeout=600)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if not data:\n",
    "            logger.warning(\"No Data, API returned no data.\")\n",
    "        else:\n",
    "            logger.info(\"API connection successful and Data is pulled\")\n",
    "            return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err}\")\n",
    "        sys.exit(1)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(\"Connection Error Could not connect to the API.\")\n",
    "        sys.exit(1)\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(\"Timeout The API request timed out.\")\n",
    "        sys.exit(1)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"API Error, An error occurred: {e}\")\n",
    "        sys.exit(1)\n",
    "    except ValueError:\n",
    "        logger.error(\"Decode Error, Failed to decode JSON from the API.\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7e754-0994-4c62-9a76-2f3ba7f541e8",
   "metadata": {},
   "source": [
    "### Fetching Data from SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9b880-b040-4b1e-bdc6-09565bfdb284",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def connect_to_sql_server(server, database, user, password, port=1433):\n",
    "    \"\"\"\n",
    "    Connect to the SQL Server.\n",
    "\n",
    "    Args:\n",
    "        string: server name\n",
    "        string: database name\n",
    "        string: User name\n",
    "        string: Password\n",
    "        Int: Port number\n",
    "\n",
    "    Returns:\n",
    "        Object: Connection object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER=<SERVER>,{port};\"\n",
    "            f\"DATABASE=<DATABASE>;\"\n",
    "            f\"UID=<USER>;\"\n",
    "            f\"PWD=<PASSWORD>\"\n",
    "        )\n",
    "        connection = pyodbc.connect(conn_str)\n",
    "        print(\"Connected to SQL Server\")\n",
    "        return connection\n",
    "    except pyodbc.Error as e:\n",
    "        print(\"Error connecting to SQL Server.\")\n",
    "        return None\n",
    "\n",
    "def fetch_sqlserver_table_as_df(connection, table_name):\n",
    "    \"\"\"\n",
    "    Fetches a SQL Server table as a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        object: connection object\n",
    "        string: Table name\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Table in the form of DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM <Schema>.<table_name>\"\n",
    "        df = pd.read_sql(query, con=connection)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching SQL Server table.\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1310eee-2edf-40b6-92ba-129287e1cf52",
   "metadata": {},
   "source": [
    "### Bulk Load and Incremental Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafbf5e-6d25-47e2-86ce-465cd0fce777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_load(engine, table_name, df):\n",
    "    \"\"\"\n",
    "    Empties the table and bulk loads new data, checking for schema compatibility.\n",
    "\n",
    "    Args:\n",
    "        engine: SQLAlchemy engine connected to the database.\n",
    "        table_name (str): The name of the table to load data into.\n",
    "        df (DataFrame): The new data to be loaded.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the DataFrame schema does not match the existing table schema.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "\n",
    "        # Add Query_status and Load_Timestamp columns\n",
    "        df['Query_status'] = 'Bulk load'\n",
    "        df['Load_Timestamp'] = datetime.now()\n",
    "\n",
    "        if inspector.has_table(table_name):\n",
    "            # Get existing table schema\n",
    "            columns = inspector.get_columns(table_name)\n",
    "            existing_columns = {col['name'] for col in columns}\n",
    "            df_columns = set(df.columns)\n",
    "\n",
    "            # Check if schemas match\n",
    "            if existing_columns != df_columns:\n",
    "                logger.error(f\"Schema mismatch for table '{table_name}'.\")\n",
    "                raise ValueError(\n",
    "                    f\"Schema mismatch for table '{table_name}'. \"\n",
    "                )\n",
    "\n",
    "            # Truncate the table\n",
    "            with engine.connect() as connection:\n",
    "                connection.execute(f\"TRUNCATE TABLE <Schema>.{table_name}\")\n",
    "                connection.commit()\n",
    "                logger.info(\"Table '<table_name>' truncated.\")\n",
    "\n",
    "        # Load data\n",
    "        df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "        logger.info(f\"Table '{table_name}' loaded with {len(df)} records.\")\n",
    "\n",
    "        # Move data (assuming this function is defined elsewhere)\n",
    "        move_stage1_to_stage2(engine, table_name, table_name, incremental=False, bulk_load=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in bulk_load for table '{table_name}': {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "\n",
    "\n",
    "def IncrementalLoadWithTracking(engine, table_name, df):\n",
    "    \"\"\"\n",
    "    Loads new records, updates status changes, and tags each row with Query_status and Load_Timestamp.\n",
    "\n",
    "    Args:\n",
    "        engine: SQLAlchemy engine connected to the database.\n",
    "        table_name (str): The name of the table to load data into.\n",
    "        df (DataFrame): The new data to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    unique_key = ''\n",
    "    lowerTableName = table_name.lower()\n",
    "\n",
    "    # Identify unique key for each table\n",
    "    if '<placeholder>' in lowerTableName:\n",
    "        unique_key = '<TABLE_ID1>'\n",
    "        status = '<TABLE_STATUS1>'\n",
    "    elif '<placeholder>' in lowerTableName:\n",
    "        unique_key = '<TABLE_ID2>'\n",
    "        status = '<TABLE_STATUS2>'\n",
    "    elif '<placeholder>' in lowerTableName:\n",
    "        unique_key = '<TABLE_ID3>'\n",
    "        status = '< TABLE_STATUS3>'\n",
    "    else:\n",
    "        logger.error(\"Incremental Load failed. No unique key found.\")\n",
    "        sys.exit()\n",
    "\n",
    "    is_incremental = False\n",
    "\n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "\n",
    "        # Deduplicate incoming data\n",
    "        # df = df.drop_duplicates(subset=unique_key, keep='last') -- Need to be revised\n",
    "\n",
    "        # Add Query_status and Load_Timestamp columns\n",
    "        df['Query_status'] = ''\n",
    "        df['Load_Timestamp'] = datetime.now()\n",
    "\n",
    "        if not inspector.has_table(table_name):\n",
    "            df['Query_status'] = 'Bulk load'\n",
    "            df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "            logger.info(f\"Table '{table_name}' created and all records inserted.\")\n",
    "            move_stage1_to_stage2(engine, table_name, table_name, incremental=False, bulk_load=True)\n",
    "            return\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            existing_df = pd.read_sql(f\"SELECT * FROM <Schema>.{table_name}\", conn)\n",
    "            filtered_df = pd.read_sql(f\"SELECT {unique_key}, {status}, Query_status, Load_Timestamp FROM <Schema>.{table_name}\", conn)\n",
    "\n",
    "        df['Load_Timestamp'] = df['Load_Timestamp'].astype('datetime64[ns]')\n",
    "        new_schema = list(zip(df.columns, df.dtypes))\n",
    "        \n",
    "        existing_schema = list(zip(existing_df.columns, existing_df.dtypes))\n",
    "            \n",
    "        if existing_df.empty:\n",
    "            if existing_schema != new_schema:\n",
    "                logger.error(f\"Schema mismatch for table '{table_name}'.\")\n",
    "                logger.error(f\"Existing schema: {existing_schema}\")\n",
    "                logger.error(f\"New schema: {new_schema}\")\n",
    "                return  # Or use sys.exit(1) if this is at the script level\n",
    "            else:\n",
    "                logger.info(\"Schema match confirmed. Proceeding with incremental load.\")\n",
    "                \n",
    "            df['Query_status'] = 'Bulk load'\n",
    "            df.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "            logger.info(f\"Initial Load: Loaded {len(df)} records into '{table_name}'.\")\n",
    "            move_stage1_to_stage2(engine, table_name, table_name, incremental=False, bulk_load=True)\n",
    "        else:\n",
    "            if existing_schema != new_schema:\n",
    "                logger.error(f\"Schema mismatch for table '{table_name}'.\")\n",
    "                logger.error(f\"Existing schema: {existing_schema}\")\n",
    "                logger.error(f\"New schema: {new_schema}\")\n",
    "                return  # Or use sys.exit(1) if this is at the script level\n",
    "            else:\n",
    "                logger.info(\"Schema match confirmed. Proceeding with incremental load.\")\n",
    "            merged_df = df.merge(filtered_df, on=unique_key, suffixes=('_new', '_old'), how='left')\n",
    "\n",
    "            # Separate into categories\n",
    "            new_rows = merged_df[merged_df[status+'_old'].isna()]\n",
    "            updated_rows = merged_df[(~merged_df[status+'_old'].isna()) & (merged_df[status+'_new'] != merged_df[status+'_old'])]\n",
    "            same_rows = merged_df[(~merged_df[status+'_old'].isna()) & (merged_df[status+'_new'] == merged_df[status+'_old'])]\n",
    "\n",
    "            timestamp = datetime.now()\n",
    "\n",
    "            # Label Query_status and timestamp\n",
    "            df.loc[df[unique_key].isin(new_rows[unique_key]), 'Query_status'] = 'newly inserted'\n",
    "            df.loc[df[unique_key].isin(updated_rows[unique_key]), 'Query_status'] = 'updated'\n",
    "            df.loc[df[unique_key].isin(same_rows[unique_key]), 'Query_status'] = 'same'\n",
    "            df['Load_Timestamp'] = timestamp\n",
    "\n",
    "            with engine.begin() as conn:\n",
    "                # Insert new records\n",
    "                if not new_rows.empty:\n",
    "                    is_incremental = True\n",
    "                    insert_data = df[df[unique_key].isin(new_rows[unique_key])]\n",
    "                    insert_data.to_sql(table_name, con=conn, if_exists='append', index=False)\n",
    "                    logger.info(f\"Inserted {len(insert_data)} new records into '{table_name}'.\")\n",
    "\n",
    "                # Update changed records\n",
    "                if not updated_rows.empty:\n",
    "                    is_incremental = True\n",
    "                    for _, row in df[df[unique_key].isin(updated_rows[unique_key])].iterrows():\n",
    "                        update_query = f\"\"\"\n",
    "                            UPDATE <Schema>.{table_name}\n",
    "                            SET {status} = {int(row[status])},\n",
    "                                Query_status = 'updated',\n",
    "                                Load_Timestamp = '{timestamp.strftime('%Y-%m-%d %H:%M:%S')}'\n",
    "                            WHERE {unique_key} = '{row[unique_key]}'\n",
    "                        \"\"\"\n",
    "                        conn.execute(text(update_query))\n",
    "                    logger.info(f\"Updated {len(updated_rows)} records in '{table_name}' due to status changes.\")\n",
    "\n",
    "                if new_rows.empty and updated_rows.empty:\n",
    "                    logger.info(f\"No new or updated records for '{table_name}'.\")\n",
    "\n",
    "            move_stage1_to_stage2(engine, table_name, table_name, incremental=is_incremental)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Insert/Update Failed:\\n\" + str(e))\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b272dc-dea6-4ede-90b8-36287cc47de6",
   "metadata": {},
   "source": [
    "### Moving Data from Stage 1 to Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972d349-d530-46a8-ae62-e61f5e6f66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_stage1_to_stage2(engine, stage1_table, stage2_table, incremental=False, bulk_load=False):\n",
    "    \"\"\"\n",
    "    Move data from Stage1 to Stage2. If incremental, only move updated/new rows.\n",
    "\n",
    "    Args:\n",
    "        engine: SQL Server connector\n",
    "        stage1_table (str): Source table (Stage1)\n",
    "        stage2_table (str): Destination table (Stage2)\n",
    "        incremental (bool): True = move only updated/new rows, False = move entire table\n",
    "        bulk_load (bool): True = move all stage 1 table into stage 2, False = only copy the schema\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            if incremental:\n",
    "                # For incremental: move only updated or newly inserted rows\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    DROP TABLE IF EXISTS <Schema>.{stage2_table};\n",
    "                    WITH MaxTimestampCTE AS (\n",
    "                        SELECT MAX(load_Timestamp) AS MaxTimestamp\n",
    "                        FROM <Schema>.{stage1_table}\n",
    "                        WHERE Query_status IN ('updated', 'newly inserted')\n",
    "                    )\n",
    "                    SELECT *\n",
    "                    INTO <Schema>.{stage2_table}\n",
    "                    FROM <Schema>.{stage1_table}\n",
    "                    WHERE Query_status IN ('updated', 'newly inserted')\n",
    "                      AND load_Timestamp = (SELECT MaxTimestamp FROM MaxTimestampCTE);\n",
    "                \"\"\"))\n",
    "                logger.info(\"Incremental Stage 2 Load: Moved updated/new data from {stage1_table} to {stage2_table}.\")\n",
    "            elif bulk_load:\n",
    "                # For bulk load: move entire table\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    DROP TABLE IF EXISTS <Schema>.{stage2_table};\n",
    "                    SELECT *\n",
    "                    INTO <Schema>.{stage2_table}\n",
    "                    FROM <Schema>.{stage1_table};\n",
    "                \"\"\"))\n",
    "                logger.info(\"Bulk Stage 2 Load: Moved full data from {stage1_table} to {stage2_table}.\")\n",
    "            else:\n",
    "                # Only copy the schema of stage 1 to stage 2\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    DROP TABLE IF EXISTS <Schema>.{stage2_table};\n",
    "                    SELECT TOP 0 *\n",
    "                    INTO <Schema>.{stage2_table}\n",
    "                    FROM <Schema>.{stage1_table};\n",
    "                \"\"\"))\n",
    "                logger.info(\"No new row added to stage 2\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Stage 2 Load Failed: Error moving data from {stage1_table} to {stage2_table}:\\n\" + str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc74c8-e27d-4c81-aff0-817638968a95",
   "metadata": {},
   "source": [
    "### Mapping Validator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e4747-e74e-4f83-90e2-482abe5785ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_unexpected_values(engine, validations):\n",
    "    \"\"\"\n",
    "    validations: a list of dictionaries\n",
    "    Each dictionary should have:\n",
    "        - name: a label for what you are checking\n",
    "        - query: the SQL query to run\n",
    "    Args:\n",
    "        engine: SQL Server Connector\n",
    "        Dictionary: Name of the query and its query\n",
    "    Return:\n",
    "        none\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as connection:\n",
    "        for validation in validations:\n",
    "            name = validation['name']\n",
    "            query = text(validation['query'])\n",
    "\n",
    "            result = connection.execute(query)\n",
    "            unexpected_values = [row[0] for row in result]\n",
    "\n",
    "            if unexpected_values:\n",
    "                logger.warning(f\"Unexpected values found for <Validation_name>: {unexpected_values}\")\n",
    "\n",
    "                # Optional: Save into a log file\n",
    "                with open(f'<Validation_name>_unexpected.log', 'a') as f:\n",
    "                    for value in unexpected_values:\n",
    "                        f.write(f\"{value}\\n\")\n",
    "            else:\n",
    "                logger.info(f\"No unexpected values found for <Validation_name>.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511e525-8488-42e0-927f-c62989cc7485",
   "metadata": {},
   "source": [
    "### Transformation Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d564f6-066c-4b7d-ad20-a854a8975243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(engine, aggregated_table):\n",
    "    \"\"\"\n",
    "    This function will transform the stage 2 data according to Sapphire API\n",
    "\n",
    "    Args:\n",
    "        engine: SQL Server Connector\n",
    "        aggregated_table: Bool Value to run aggregated tables\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    # Validations of the Mapping that needs to be done\n",
    "    validations = [\n",
    "        {\n",
    "            \"name\": \"Column1 For Table1\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column1\n",
    "                FROM Table1\n",
    "                WHERE Column1 NOT IN ('value1','value2','value3','value4','value5');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column2 For Table1\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column2\n",
    "                FROM Table1\n",
    "                WHERE Column2 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column3 For Table1\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column3\n",
    "                FROM Table1\n",
    "                WHERE Column3 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column4 For Table1\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column4\n",
    "            FROM Table1\n",
    "            WHERE Column4 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column1 For Table2\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column1\n",
    "                FROM Table2\n",
    "                WHERE Column1 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column2 For Table2\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column2\n",
    "                FROM Table2\n",
    "                WHERE Column2 NOT IN ('value1','value2','value3','value4','value5');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column1 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column1\n",
    "                FROM Table3\n",
    "                WHERE Column1 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column2 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "                SELECT DISTINCT Column2\n",
    "                FROM Table3\n",
    "                WHERE Column2 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column3 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column3\n",
    "            FROM Table3\n",
    "            WHERE Column3 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column4 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column4\n",
    "            FROM Table3\n",
    "            WHERE Column4 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column5 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column5\n",
    "            FROM Table3\n",
    "            WHERE Column5 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column6 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column6\n",
    "            FROM Table3\n",
    "            WHERE Column6 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14');\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Column7 For Table3\",\n",
    "            \"query\": \"\"\"\n",
    "            SELECT DISTINCT Column7\n",
    "            FROM Table3\n",
    "            WHERE Column7 NOT IN ('value1','value2','value3','value4','value5','value6','value7','value8','value9','value10','value11','value12','value13','value14','value15','value16','value17','value18','value19','value20');\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    \n",
    "    mapping_query = text(\"\"\"\n",
    "        ----Making lookups\n",
    "        ----=================================================================================\n",
    "        DROP TABLE IF EXISTS Table_01;\n",
    "        CREATE TABLE Table_01 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT,\n",
    "            Col_03 INT\n",
    "        );\n",
    "        INSERT INTO Table_01 (Col_01, Col_02, Col_03) VALUES\n",
    "            (1, 1, 1), (1, 2, 2), (1, 3, 3), (1, 4, 4), (1, 5, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_02;\n",
    "        CREATE TABLE Table_02 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT,\n",
    "            Col_03 INT,\n",
    "            Col_04 INT\n",
    "        );\n",
    "        INSERT INTO Table_02 (Col_01, Col_02, Col_03, Col_04) VALUES\n",
    "            (1, 1, 1, 1), (1, 1, 2, 2), (1, 2, 3, 3), (1, 2, 4, 4), (1, 2, 5, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_03;\n",
    "        CREATE TABLE Table_03 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_03 (Col_01, Col_02) VALUES\n",
    "            (1, 1), (2, 2), (3, 3), (4, 4), (5, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_04;\n",
    "        CREATE TABLE Table_04 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_04 (Col_01, Col_02) VALUES\n",
    "            (1, NULL), (2, NULL), (3, 1), (3, 2), (4, 3);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_05;\n",
    "        CREATE TABLE Table_05 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_05 (Col_01, Col_02) VALUES\n",
    "            (0, NULL), (1, 1), (2, 2), (3, 3), (3, 4);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_06;\n",
    "        CREATE TABLE Table_06 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_06 (Col_01, Col_02) VALUES\n",
    "            (0, NULL), (1, 1), (2, 2), (3, NULL), (4, 3);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_07;\n",
    "        CREATE TABLE Table_07 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_07 (Col_01, Col_02) VALUES\n",
    "            (0, 1), (0, 2), (0, 3), (1, 4), (2, 5);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_08;\n",
    "        CREATE TABLE Table_08 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_08 (Col_01, Col_02) VALUES\n",
    "            (1, NULL), (2, NULL), (3, NULL), (4, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_09;\n",
    "        CREATE TABLE Table_09 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_09 (Col_01, Col_02) VALUES\n",
    "            (0, 0), (1, 1), (2, 2), (3, 3);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_10;\n",
    "        CREATE TABLE Table_10 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_10 (Col_01, Col_02) VALUES\n",
    "            (0, 0), (1, 1), (2, 2), (3, 3), (4, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_11;\n",
    "        CREATE TABLE Table_11 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_11 (Col_01, Col_02) VALUES\n",
    "            (0, 0), (1, 1), (2, 2), (3, 3), (4, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_12;\n",
    "        CREATE TABLE Table_12 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_12 (Col_01, Col_02) VALUES\n",
    "            (3, 1), (4, 2), (5, 3), (6, 4), (7, 5);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_13;\n",
    "        CREATE TABLE Table_13 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_13 (Col_01, Col_02) VALUES\n",
    "            (101, NULL), (102, NULL), (103, NULL), (104, 1), (105, NULL);\n",
    "\n",
    "        DROP TABLE IF EXISTS Table_14;\n",
    "        CREATE TABLE Table_14 (\n",
    "            Col_01 INT,\n",
    "            Col_02 INT\n",
    "        );\n",
    "        INSERT INTO Table_14 (Col_01, Col_02) VALUES\n",
    "            (0, 1), (1, 2), (2, 3);\n",
    "\n",
    "    \"\"\") \n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # 1. Mapping Tables\n",
    "            if not aggregated_table:\n",
    "                # Run the Mapping Validator Function\n",
    "                check_unexpected_values(engine, validations)\n",
    "                try:\n",
    "                    conn.execute(mapping_query)\n",
    "                    logger.info(\"Mapping Tables Created\")\n",
    "                except SQLAlchemyError as e:\n",
    "                    logger.error(\"Failed to execute mapping_query.\")\n",
    "                    logger.error(str(e))\n",
    "\n",
    "            # 2. Table01-related tables\n",
    "            try:\n",
    "                result = conn.execute(text(\"SELECT COUNT(*) FROM Schema01.Table01\"))\n",
    "                if result.scalar() > 0:\n",
    "                    table01_queries = []\n",
    "                    if not aggregated_table:\n",
    "                        table01_queries.append((query_Table01, \"Table01\"))\n",
    "                    else:\n",
    "                        table01_queries.extend([\n",
    "                            (query_Table02, \"Table02\"),\n",
    "                            (query_Table03, \"Table03\"),\n",
    "                            (query_Table04, \"Table04\"),\n",
    "                            (query_Table05, \"Table05\"),\n",
    "                            (query_Table06, \"Table06\"),\n",
    "                            (query_Table07, \"Table07\"),\n",
    "                            (query_Table08, \"Table08\"),\n",
    "                            (query_Table09, \"Table09\"),\n",
    "                        ])\n",
    "                    for query, name in table01_queries:\n",
    "                        try:\n",
    "                            conn.execute(query)\n",
    "                            logger.info(f\"{name} Table Created\")\n",
    "                        except SQLAlchemyError as e:\n",
    "                            logger.error(f\"Failed to create {name} table: {e}\")\n",
    "            except SQLAlchemyError as e:\n",
    "                logger.error(\"Error checking/processing Table01 data.\")\n",
    "                logger.error(str(e))\n",
    "\n",
    "            # 3. Table02-related tables\n",
    "            try:\n",
    "                result = conn.execute(text(\"SELECT COUNT(*) FROM Schema01.Table02\"))\n",
    "                if result.scalar() > 0:\n",
    "                    table02_queries = []\n",
    "                    if not aggregated_table:\n",
    "                        table02_queries.append((query_Table10, \"Table10\"))\n",
    "                    else:\n",
    "                        table02_queries.extend([\n",
    "                            (query_Table11, \"Table11\"),\n",
    "                            (query_Table12, \"Table12\"),\n",
    "                            (query_Table13, \"Table13\"),\n",
    "                            (query_Table14, \"Table14\"),\n",
    "                            (query_Table15, \"Table15\"),\n",
    "                            (query_Table16, \"Table16\"),\n",
    "                        ])\n",
    "                    for query, name in table02_queries:\n",
    "                        try:\n",
    "                            conn.execute(query)\n",
    "                            logger.info(f\"{name} Table Created\")\n",
    "                        except SQLAlchemyError as e:\n",
    "                            logger.error(f\"Failed to create {name} table: {e}\")\n",
    "            except SQLAlchemyError as e:\n",
    "                logger.error(\"Error checking/processing Table02 data.\")\n",
    "                logger.error(str(e))\n",
    "\n",
    "            # 4. Table03-related tables\n",
    "            try:\n",
    "                result = conn.execute(text(\"SELECT COUNT(*) FROM Schema01.Table03\"))\n",
    "                if result.scalar() > 0:\n",
    "                    table03_queries = []\n",
    "                    if not aggregated_table:\n",
    "                        table03_queries.append((query_Table17, \"Table17\"))\n",
    "                    else:\n",
    "                        table03_queries.extend([\n",
    "                            (query_Table18, \"Table18\"),\n",
    "                            (query_Table19, \"Table19\"),\n",
    "                            (query_Table20, \"Table20\"),\n",
    "                            (query_Table21, \"Table21\"),\n",
    "                            (query_Table22, \"Table22\"),\n",
    "                            (query_Table23, \"Table23\"),\n",
    "                        ])\n",
    "                    for query, name in table03_queries:\n",
    "                        try:\n",
    "                            conn.execute(query)\n",
    "                            logger.info(f\"{name} Table Created\")\n",
    "                        except SQLAlchemyError as e:\n",
    "                            logger.error(f\"Failed to create {name} table: {e}\")\n",
    "            except SQLAlchemyError as e:\n",
    "                logger.error(\"Error checking/processing Table03 data.\")\n",
    "                logger.error(str(e))\n",
    "\n",
    "        logger.info(\"All possible queries executed (errors logged if any).\")\n",
    "        return\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.critical(\"Critical DB error â€” transaction not even started.\")\n",
    "        logger.critical(str(e))\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d101c0-0faf-4824-89ad-f74f2ab7a8a8",
   "metadata": {},
   "source": [
    "### Moving to Stage3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0073662-4fe9-4b6f-89ca-d1c91b238919",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def move_stage2_to_stage3(engine, table2_name, table3_name, aggregated_table = False):\n",
    "    \"\"\"\n",
    "    Copies data from Stage2 table to Stage3 table. If Stage3 table does not exist, it is created with the same schema.\n",
    "\n",
    "    Args:\n",
    "        engine: SQLAlchemy engine connected to SQL Server\n",
    "        table2_name (str): Source table name (Stage2 schema)\n",
    "        table3_name (str): Target table name (Stage3 schema)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            if not aggregated_table:\n",
    "                # Step 1: Check if the table exists in Stage3 database\n",
    "                table_exists = conn.execute(text(f\"\"\"\n",
    "                    SELECT 1 \n",
    "                    FROM Stage3.INFORMATION_SCHEMA.TABLES \n",
    "                    WHERE TABLE_NAME = :table_name AND TABLE_SCHEMA = 'dbo'\n",
    "                \"\"\"), {'table_name': table3_name}).scalar()\n",
    "    \n",
    "                if not table_exists:\n",
    "                    # Step 2: Create the table structure in Stage3\n",
    "                    conn.execute(text(f\"\"\"\n",
    "                        SELECT * INTO Stage3.dbo.{table3_name}\n",
    "                        FROM Stage2.dbo.{table2_name}\n",
    "                        WHERE 1 = 0\n",
    "                    \"\"\"))\n",
    "    \n",
    "                # Step 3: Insert data from Stage2 to Stage3\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    INSERT INTO Stage3.dbo.{table3_name}\n",
    "                    SELECT * FROM Stage2.dbo.{table2_name}\n",
    "                \"\"\"))\n",
    "            else:\n",
    "                # Step 1: Check if the table exists in Stage3 database\n",
    "                table_exists = conn.execute(text(f\"\"\"\n",
    "                    SELECT 1 \n",
    "                    FROM Stage3.INFORMATION_SCHEMA.TABLES \n",
    "                    WHERE TABLE_NAME = :table_name AND TABLE_SCHEMA = 'dbo'\n",
    "                \"\"\"), {'table_name': table3_name}).scalar()\n",
    "    \n",
    "                if not table_exists:\n",
    "                    # Step 2: Create the table structure in Stage3\n",
    "                    conn.execute(text(f\"\"\"\n",
    "                        SELECT * INTO Stage3.dbo.{table3_name}\n",
    "                        FROM Stage2.dbo.{table2_name}\n",
    "                        WHERE 1 = 0\n",
    "                    \"\"\"))\n",
    "    \n",
    "                # Step 3: Insert data from Stage2 to Stage3\n",
    "                conn.execute(text(f\"\"\"\n",
    "                    TRUNCATE TABLE Stage3.dbo.{table3_name}\n",
    "                    INSERT INTO Stage3.dbo.{table3_name}\n",
    "                    SELECT * FROM Stage2.dbo.{table2_name}\n",
    "                \"\"\"))\n",
    "\n",
    "        logger.info(f\"Stage 3 Load: Data from {table2_name} inserted into {table3_name} successfully.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Stage 3 Load Failed: Error processing tables:\\n{e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6748b6-e3a0-43f8-8c72-15ff852063a4",
   "metadata": {},
   "source": [
    "#### Data Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f718895-3acf-4850-8950-124b7ff9d84e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data_summary(engine, table_name):\n",
    "    \"\"\"\n",
    "    Generates the data summary of the main tables.\n",
    "\n",
    "    Args:\n",
    "        engine: SQLAlchemy engine connected to SQL Server\n",
    "        table_name (str): table name for which data summary will be generated\n",
    "        output_path (str): path where the file which is saved\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    output_path = table_name + \"_Data_Summary.txt\"\n",
    "    logger.info(\"Generating the Data Summary\")\n",
    "    \n",
    "    if table_name.lower() == \"school\":\n",
    "        queries =  {\n",
    "            \"query_01\": \"SELECT COUNT(*) AS Col_01 FROM Stage1.dbo.Tbl_01\",\n",
    "            \"query_02\": \"SELECT COUNT(DISTINCT Col_02) AS Col_03 FROM Stage1.dbo.Tbl_01\",\n",
    "            \"query_03\": \"\"\"\n",
    "                SELECT COALESCE(SUM(Col_04), 0) AS Col_05 FROM (\n",
    "                    SELECT COUNT(*) as Col_04\n",
    "                    FROM Stage1.dbo.Tbl_01\n",
    "                    GROUP BY Col_02\n",
    "                    HAVING COUNT(*) > 1\n",
    "                ) AS TblX\n",
    "            \"\"\",\n",
    "            \"query_04\": \"\"\"\n",
    "                SELECT Col_06, COUNT(Col_02) as Col_07\n",
    "                FROM Stage1.dbo.Tbl_01\n",
    "                GROUP BY Col_06\n",
    "            \"\"\",\n",
    "            \"query_05\": \"\"\"\n",
    "                SELECT Col_08, COUNT(Col_02) as Col_09\n",
    "                FROM Stage1.dbo.Tbl_01\n",
    "                GROUP BY Col_08\n",
    "            \"\"\",\n",
    "            \"query_06\": \"\"\"\n",
    "                SELECT Col_10, COUNT(Col_02) as Col_11\n",
    "                FROM Stage1.dbo.Tbl_01 ns\n",
    "                GROUP BY Col_10\n",
    "            \"\"\",\n",
    "            \"query_07\": \"\"\"\n",
    "                SELECT Col_08, COUNT(Col_12) as Col_13\n",
    "                FROM Stage1.dbo.Tbl_01 ns\n",
    "                GROUP BY Col_12\n",
    "            \"\"\",\n",
    "            \"query_08\": \"\"\"\n",
    "                SELECT Col_08, COUNT(Col_14) as Col_15\n",
    "                FROM Stage1.dbo.Tbl_01 ns\n",
    "                GROUP BY Col_14\n",
    "            \"\"\",\n",
    "            \"query_09\": \"\"\"\n",
    "                SELECT Col_08, COUNT(Col_16) as Col_17\n",
    "                FROM Stage1.dbo.Tbl_01 ns\n",
    "                GROUP BY Col_16\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "    if table_name.lower() == \"teachers\":\n",
    "        output_path = table_name + \"_Data_Summary.txt\"\n",
    "        queries =  {\n",
    "            \"query_01\": \"\"\"\n",
    "                SELECT COUNT(*) AS Col_01\n",
    "                FROM Stage1.dbo.Tbl_02;\n",
    "            \"\"\",\n",
    "            \"query_02\": \"\"\"\n",
    "                SELECT COUNT(DISTINCT Col_02) AS Col_03\n",
    "                FROM Stage1.dbo.Tbl_02;\n",
    "            \"\"\",\n",
    "            \"query_03\": \"\"\"\n",
    "                SELECT COALESCE(SUM(Col_04), 0) AS Col_05\n",
    "                FROM (\n",
    "                    SELECT COUNT(*) AS Col_04\n",
    "                    FROM Stage1.dbo.Tbl_02\n",
    "                    GROUP BY Col_02\n",
    "                    HAVING COUNT(*) > 1\n",
    "                ) AS TblY;\n",
    "            \"\"\",\n",
    "            \"query_04\": \"\"\"\n",
    "                SELECT Col_06, COUNT(Col_02) AS Col_07\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_06;\n",
    "            \"\"\",\n",
    "            \"query_05\": \"\"\"\n",
    "                SELECT Col_08, COUNT(Col_02) AS Col_09\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_08;\n",
    "            \"\"\",\n",
    "            \"query_06\": \"\"\"\n",
    "                SELECT Col_10, COUNT(Col_02) AS Col_11\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_10;\n",
    "            \"\"\",\n",
    "            \"query_07\": \"\"\"\n",
    "                SELECT Col_12, COUNT(Col_02) AS Col_13\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_12;\n",
    "            \"\"\",\n",
    "            \"query_08\": \"\"\"\n",
    "                SELECT Col_14, COUNT(Col_02) AS Col_15\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_14;\n",
    "            \"\"\",\n",
    "            \"query_09\": \"\"\"\n",
    "                SELECT Col_16, COUNT(Col_02) AS Col_17\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_16;\n",
    "            \"\"\",\n",
    "            \"query_10\": \"\"\"\n",
    "                SELECT Col_06, ROUND(AVG(CAST(Col_18 AS FLOAT)), 2) AS Col_19\n",
    "                FROM Stage1.dbo.Tbl_02\n",
    "                GROUP BY Col_06;\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "    if table_name.lower() == \"learners\":\n",
    "        output_path = table_name + \"_Data_Summary.txt\"\n",
    "        queries =  {\n",
    "            \"query_01\": \"SELECT COUNT(*) AS Col_01 FROM Stage1.dbo.Tbl_03;\",\n",
    "            \"query_02\": \"SELECT COUNT(DISTINCT Col_02) AS Col_03 FROM Stage1.dbo.Tbl_03;\",\n",
    "            \"query_03\": \"SELECT COALESCE(SUM(Col_04), 0) AS Col_05 FROM (SELECT COUNT(*) AS Col_04 FROM Stage1.dbo.Tbl_03 GROUP BY Col_02 HAVING COUNT(*) > 1) AS TblZ;\",\n",
    "            \"query_04\": \"SELECT Col_06, COUNT(Col_02) AS Col_07 FROM Stage1.dbo.Tbl_03 GROUP BY Col_06;\",\n",
    "            \"query_05\": \"SELECT Col_08, COUNT(Col_02) AS Col_09 FROM Stage1.dbo.Tbl_03 GROUP BY Col_08;\",\n",
    "            \"query_06\": \"SELECT Col_10, COUNT(Col_02) AS Col_11 FROM Stage1.dbo.Tbl_03 GROUP BY Col_10;\",\n",
    "            \"query_07\": \"SELECT Col_12, COUNT(Col_02) AS Col_13 FROM Stage1.dbo.Tbl_03 GROUP BY Col_12;\",\n",
    "            \"query_08\": \"SELECT Col_14, COUNT(Col_02) AS Col_15 FROM Stage1.dbo.Tbl_03 GROUP BY Col_14;\",\n",
    "            \"query_09\": \"SELECT Col_16, COUNT(Col_02) AS Col_17 FROM Stage1.dbo.Tbl_03 GROUP BY Col_16;\",\n",
    "            \"query_10\": \"SELECT Col_18, COUNT(Col_02) AS Col_19 FROM Stage1.dbo.Tbl_03 GROUP BY Col_18;\",\n",
    "            \"query_11\": \"SELECT Col_20, COUNT(Col_02) AS Col_21 FROM Stage1.dbo.Tbl_03 GROUP BY Col_20;\",\n",
    "            \"query_12\": \"SELECT Col_22, COUNT(Col_02) AS Col_23 FROM Stage1.dbo.Tbl_03 GROUP BY Col_22;\",\n",
    "            \"query_13\": \"SELECT Col_24, COUNT(Col_02) AS Col_25 FROM Stage1.dbo.Tbl_03 GROUP BY Col_24;\",\n",
    "            \"query_14\": \"SELECT Col_26, COUNT(Col_02) AS Col_27 FROM Stage1.dbo.Tbl_03 GROUP BY Col_26;\",\n",
    "            \"query_15\": \"SELECT Col_28, COUNT(Col_02) AS Col_29 FROM Stage1.dbo.Tbl_03 GROUP BY Col_28;\",\n",
    "            \"query_16\": \"SELECT Col_30, COUNT(Col_02) AS Col_31 FROM Stage1.dbo.Tbl_03 GROUP BY Col_30;\",\n",
    "            \"query_17\": \"SELECT Col_32, COUNT(Col_02) AS Col_33 FROM Stage1.dbo.Tbl_03 GROUP BY Col_32;\",\n",
    "            \"query_18\": \"SELECT YEAR(Col_34) AS Col_35, COUNT(Col_02) AS Col_36 FROM Stage1.dbo.Tbl_03 WHERE Col_34 IS NOT NULL GROUP BY YEAR(Col_34) ORDER BY Col_35;\",\n",
    "            \"query_19\": \"SELECT Col_37, COUNT(Col_02) AS Col_38 FROM Stage1.dbo.Tbl_03 GROUP BY Col_37;\"\n",
    "        }\n",
    "\n",
    "    with engine.connect() as conn, open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        f.write(f\"\\n========= Summary for {table_name} at {timestamp} =========\\n\")\n",
    "\n",
    "        for title, query in queries.items():\n",
    "            f.write(f\"\\n-- {title} --\\n\")\n",
    "            try:\n",
    "                result = conn.execute(text(query))\n",
    "                rows = result.fetchall()\n",
    "                columns = result.keys()\n",
    "\n",
    "                if rows:\n",
    "                    table = tabulate(rows, headers=columns, tablefmt=\"grid\")\n",
    "                    f.write(f\"{table}\\n\")\n",
    "                else:\n",
    "                    f.write(\"No data returned.\\n\")\n",
    "            except Exception as e:\n",
    "                f.write(f\"Query failed: {e}\\n\")\n",
    "\n",
    "        f.write(f\"\\n========== End of {table_name} Summary ==========\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340a478-8946-43da-a1c0-9f2e35ba934c",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf771b-e7e3-4930-b50b-30184dcc8f86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_cases(engine, table_name):\n",
    "    \"\"\"\n",
    "    Run the test cases for the ETL pipeline (anonymized)\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine connected to SQL Server\n",
    "        table_name (str): table name for which data summary will be generated\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # List for storing test case results\n",
    "    results = []\n",
    "    results.append((\"-------------- Running Tests at \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \" --------------\", \"For Table \" + table_name))\n",
    "    \n",
    "    if table_name.lower() == 'table_01':\n",
    "        # Test no 1: Check whether the stage 3 table is null or not?\n",
    "        query = f\"SELECT COUNT(*) AS row_count FROM Stage3.dbo.{table_name};\"\n",
    "        result = pd.read_sql(query, engine)\n",
    "        row_count = result.iloc[0]['row_count']\n",
    "        if row_count == 0:\n",
    "            results.append((f\"Test no 1 : Stage 3 {table_name} is not empty\", \"FAIL\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 1 : Stage 3 {table_name} is not empty\", \"PASS\"))\n",
    "        \n",
    "        # Test no 2: Count of distinct primary key in Stage 1\n",
    "        query = f\"\"\"SELECT COUNT(DISTINCT col_01) AS count FROM Stage1.dbo.{table_name} where col_04 != 'val_01' AND col_04 != 'val_02' AND col_04 != 'val_03' AND col_07 = 1\"\"\"\n",
    "        stage1_count = pd.read_sql(query, engine).iloc[0]['count']\n",
    "        stage3_count = pd.read_sql(f\"SELECT COUNT(DISTINCT col_05) AS count FROM Stage3.dbo.{table_name}\", engine).iloc[0]['count']\n",
    "        if stage1_count == stage3_count:\n",
    "            results.append((f\"Test no 2 : Stage1 vs Stage 3 primary key Count for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 2 : Stage1 vs Stage 3 primary key Count for {table_name}\", \"FAIL\"))\n",
    "        \n",
    "        # Test no 3: Check the count of the Province in the final table\n",
    "        query = f\"SELECT COUNT(DISTINCT col_02) AS province_count FROM Stage3.dbo.{table_name}\"\n",
    "        province_count = pd.read_sql(query, engine)\n",
    "        output = province_count.iloc[0]['province_count']\n",
    "        if output == 1:\n",
    "            results.append((f\"Test no 3: Stage 3 Province Count is 1 in {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 3: Stage 3 Province Count is 1 in {table_name}\", \"FAIL\"))\n",
    "        \n",
    "        # Test no 4: Check the given Data types and column names from the Output Table\n",
    "        expected_schema = {\n",
    "            \"col_01\": \"NVARCHAR\",\n",
    "            \"col_03\": \"SMALLINT\",\n",
    "            \"col_02\": \"TINYINT\",\n",
    "            \"col_05\": \"NVARCHAR\",\n",
    "            \"col_06\": \"NVARCHAR\",\n",
    "            \"col_08\": \"NVARCHAR\",\n",
    "            \"col_09\": \"NVARCHAR\",\n",
    "            \"col_10\": \"INT\",\n",
    "            \"col_11\": \"NVARCHAR\",\n",
    "            \"col_12\": \"NVARCHAR\",\n",
    "            \"col_13\": \"INT\",\n",
    "            \"col_14\": \"INT\",\n",
    "            \"col_15\": \"DATE\",\n",
    "            \"col_16\": \"TINYINT\",\n",
    "            \"col_17\": \"SMALLINT\",\n",
    "            \"col_18\": \"TINYINT\",\n",
    "            \"col_19\": \"NVARCHAR\",\n",
    "            \"col_20\": \"TINYINT\",\n",
    "            \"col_21\": \"TINYINT\",\n",
    "            \"col_22\": \"TINYINT\",\n",
    "            \"col_23\": \"DATE\",\n",
    "            \"col_24\": \"TINYINT\",\n",
    "            \"col_25\": \"SMALLINT\",\n",
    "            \"col_26\": \"TINYINT\",\n",
    "            \"col_27\": \"TINYINT\",\n",
    "            \"col_28\": \"TINYINT\",\n",
    "            \"col_29\": \"TINYINT\",\n",
    "            \"col_30\": \"NVARCHAR\",\n",
    "            \"col_31\": \"NVARCHAR\",\n",
    "            \"col_32\": \"NVARCHAR\",\n",
    "            \"col_33\": \"NVARCHAR\"\n",
    "        }\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT c.name AS column_name, ty.name AS data_type\n",
    "            FROM Stage3.sys.columns c\n",
    "            JOIN Stage3.sys.tables t ON c.object_id = t.object_id\n",
    "            JOIN Stage3.sys.schemas s ON t.schema_id = s.schema_id\n",
    "            JOIN Stage3.sys.types ty ON c.user_type_id = ty.user_type_id\n",
    "            JOIN sys.databases db ON db.name = 'Stage3'\n",
    "            WHERE t.name = '{table_name}'\n",
    "        ) inner_query\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        actual_schema = dict(zip(df['column_name'], df['data_type']))\n",
    "        expected_keys_lower = [k.lower() for k in expected_schema.keys()]\n",
    "        actual_keys_lower = [k.lower() for k in actual_schema.keys()]\n",
    "        if expected_keys_lower != actual_keys_lower:\n",
    "            results.append((f\"Test no 4 PART 1: Stage 3 Column Name matched (case-insensitive) for {table_name}\", \"FAIL\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 4 PART 1: Stage 3 Column Name matched (case-insensitive) for {table_name}\", \"PASS\"))\n",
    "        mismatches = []\n",
    "        for col, expected_type in expected_schema.items():\n",
    "            actual_type = actual_schema.get(col)\n",
    "            if actual_type.lower() != expected_type.lower():\n",
    "                mismatches.append((col, expected_type, actual_type))\n",
    "        if not mismatches:\n",
    "            results.append((f\"Test no 4 PART 2: Stage 3 Column Data Type matched (case-insensitive) for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 4 PART 2: Stage 3 Column Data Type matched (case-insensitive) for {table_name}\", \"FAIL\"))\n",
    "        \n",
    "        # Test no 5: District Unique Count\n",
    "        query = f\"SELECT COUNT(DISTINCT col_34) AS count FROM Stage1.dbo.{table_name} where col_04 != 'val_01' AND col_04 != 'val_02' AND col_04 != 'val_03' AND col_07 = 1\"\"\"\n",
    "        stage1_count = pd.read_sql(query, engine).iloc[0]['count']\n",
    "        stage3_count = pd.read_sql(f\"SELECT COUNT(DISTINCT col_13) AS count FROM Stage3.dbo.{table_name}\", engine).iloc[0]['count']\n",
    "        if stage1_count == stage3_count:\n",
    "            results.append((f\"Test no 5 Part 1 : Stage1 vs Stage 3 District Unique Count for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 5 Part 1 : Stage1 vs Stage 3 District Unique Count for {table_name}\", \"FAIL\"))\n",
    "        \n",
    "        query = f\"SELECT COUNT(DISTINCT col_35) AS count FROM Stage1.dbo.{table_name} where col_04 != 'val_01' AND col_04 != 'val_02' AND col_04 != 'val_03' AND col_07 = 1\"\"\"\n",
    "        stage1_count = pd.read_sql(query, engine).iloc[0]['count']\n",
    "        stage3_count = pd.read_sql(f\"SELECT COUNT(DISTINCT col_14) AS count FROM Stage3.dbo.{table_name}\", engine).iloc[0]['count']\n",
    "        if stage1_count == stage3_count:\n",
    "            results.append((f\"Test no 5 Part 2 : Stage1 vs Stage 3 Tehsil Unique Count for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 5 Part 2 : Stage1 vs Stage 3 Tehsil Unique Count for {table_name}\", \"FAIL\"))\n",
    "        \n",
    "        # Test no 6: Mapping Validity Checks\n",
    "        # Province Code Mapping\n",
    "        query = f\"\"\"\n",
    "            SELECT DISTINCT CASE WHEN Inner_query.Final_col_02 = l1.col_02 THEN 'TRUE' ELSE 'FALSE' END AS [check]\n",
    "            FROM Stage2.dbo.Lookup_01 l1\n",
    "            RIGHT JOIN (\n",
    "                SELECT DISTINCT t1.col_02 as Final_col_02, t2.col_36 as Initial_col_36\n",
    "                FROM Stage3.dbo.Table_01 t1\n",
    "                LEFT JOIN Stage1.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    "            ) Inner_query ON Inner_query.Initial_col_36 = l1.col_37\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        if df['check'].astype(str).str.lower().eq('true').any():\n",
    "            results.append((f\"Test no 6 Part 1 : Province Mapping Validity Check for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 6 Part 1 : Province Mapping Validity Check for {table_name}\", \"FAIL\"))\n",
    "        # School Level Mapping\n",
    "        query = f\"\"\"\n",
    "            SELECT DISTINCT CASE WHEN Inner_query.Final_col_16 = l2.col_16 THEN 'TRUE' ELSE 'FALSE' END AS [check]\n",
    "            FROM Stage2.dbo.Lookup_02 l2\n",
    "            RIGHT JOIN (\n",
    "                SELECT DISTINCT t1.col_16 as Final_col_16, t2.col_38 as Initial_col_38\n",
    "                FROM Stage3.dbo.Table_01 t1\n",
    "                LEFT JOIN Stage1.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    "            ) Inner_query ON Inner_query.Initial_col_38 = l2.col_39\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        if df['check'].astype(str).str.lower().eq('true').any():\n",
    "            results.append((f\"Test no 6 Part 2 : School Level Mapping Validity Check for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 6 Part 2 : School Level Mapping Validity Check for {table_name}\", \"FAIL\"))\n",
    "        # Gender Mapping\n",
    "        query = f\"\"\"\n",
    "            SELECT DISTINCT CASE WHEN Inner_query.Final_col_27 = l3.col_27 THEN 'TRUE' ELSE 'FALSE' END AS [check]\n",
    "            FROM Stage2.dbo.Lookup_03 l3\n",
    "            RIGHT JOIN (\n",
    "                SELECT DISTINCT t1.col_27 as Final_col_27, t2.col_40 as Initial_col_40\n",
    "                FROM Stage3.dbo.Table_01 t1\n",
    "                LEFT JOIN Stage1.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    "            ) Inner_query ON Inner_query.Initial_col_40 = l3.col_41\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        if df['check'].astype(str).str.lower().eq('true').any():\n",
    "            results.append((f\"Test no 6 Part 3 : Gender Mapping Validity Check for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 6 Part 3 : Gender Mapping Validity Check for {table_name}\", \"FAIL\"))\n",
    "        # Medium Mapping\n",
    "        query = f\"\"\"\n",
    "            SELECT DISTINCT CASE WHEN Inner_query.Final_col_29 = l4.col_29 THEN 'TRUE' ELSE 'FALSE' END AS [check]\n",
    "            FROM Stage2.dbo.Lookup_04 l4\n",
    "            RIGHT JOIN (\n",
    "                SELECT DISTINCT t1.col_29 as Final_col_29, t2.col_42 as Initial_col_42\n",
    "                FROM Stage3.dbo.Table_01 t1\n",
    "                LEFT JOIN Stage1.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    "            ) Inner_query ON Inner_query.Initial_col_42 = l4.col_43\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        if df['check'].astype(str).str.lower().eq('true').any():\n",
    "            results.append((f\"Test no 6 Part 4 : Medium Mapping Validity Check for {table_name}\", \"PASS\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 6 Part 4 : Medium Mapping Validity Check for {table_name}\", \"FAIL\"))\n",
    "        # Test no 7: Date format check\n",
    "        column_name = 'col_15'\n",
    "        query = f\"SELECT {column_name} FROM Stage3.dbo.Table_01 WHERE {column_name} IS NOT NULL\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        def is_yyyy_mm_dd_format(date_val):\n",
    "            try:\n",
    "                date_str = str(date_val)\n",
    "                return date_str == datetime.strftime(pd.to_datetime(date_val), '%Y-%m-%d')\n",
    "            except Exception:\n",
    "                return False\n",
    "        df['is_valid_format'] = df[column_name].apply(is_yyyy_mm_dd_format)\n",
    "        not_formated_row = df[~df['is_valid_format']]\n",
    "        if not not_formated_row.empty:\n",
    "            results.append((f\"Test no 7 Part 1 : Date format for column {column_name}, Check for {table_name}\", \"FAIL\"))\n",
    "        else:\n",
    "            results.append((f\"Test no 7 Part 1 : Date format for column {column_name}, Check for {table_name}\", \"PASS\"))\n",
    "    \n",
    "    # ...repeat similar anonymization for Table_02 and Table_03 blocks...\n",
    "    # ...existing code for teachers and learners, anonymized in the same way...\n",
    "    \n",
    "    # General Test: Validity of join between Table_02 and Table_01\n",
    "    query = f\"\"\"SELECT COUNT (DISTINCT Table_02_PID) as row_count FROM (\n",
    "            SELECT t1.col_01 as Table_02_PID, t2.col_01 as Table_01_PID\n",
    "            FROM Stage3.dbo.Table_02 t1\n",
    "            LEFT JOIN Stage3.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    ") X where Table_01_PID IS NULL\"\"\"\n",
    "    query_result = pd.read_sql(query, engine)\n",
    "    output = query_result.iloc[0]['row_count']\n",
    "    if output == 0:\n",
    "        results.append((f\"General Test: Table_02-Table_01 join validity in Stage 3\", \"PASS\"))\n",
    "    else:\n",
    "        results.append((f\"General Test: Table_02-Table_01 join validity in Stage 3\", \"FAIL\"))\n",
    "    \n",
    "    # General Test: Validity of join between Table_03 and Table_01\n",
    "    query = f\"\"\"SELECT COUNT (DISTINCT Table_03_PID) as row_count FROM (\n",
    "            SELECT t1.col_01 as Table_03_PID, t2.col_01 as Table_01_PID\n",
    "            FROM Stage3.dbo.Table_03 t1\n",
    "            LEFT JOIN Stage3.dbo.Table_01 t2 ON t1.col_01 = t2.col_01\n",
    ") X where Table_01_PID IS NULL\"\"\"\n",
    "    query_result = pd.read_sql(query, engine)\n",
    "    output = query_result.iloc[0]['row_count']\n",
    "    if output == 0:\n",
    "        results.append((f\"General Test: Table_03-Table_01 join validity in Stage 3\", \"PASS\"))\n",
    "    else:\n",
    "        results.append((f\"General Test: Table_03-Table_01 join validity in Stage 3\", \"FAIL\"))\n",
    "    \n",
    "    output_path = \"test_cases_results.txt\"\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for description, status in results:\n",
    "            f.write(f\"{description} : {status}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c079fe-efa6-4cc3-8384-39ef34c5f1db",
   "metadata": {},
   "source": [
    "# Main Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18481219-2bf6-4dd7-9ce8-f5d2437cae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from tkinter import Tk, messagebox\n",
    "from sqlalchemy import inspect\n",
    "import logging\n",
    "import sys\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from datetime import datetime\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pyodbc\n",
    "from tabulate import tabulate\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# === Logging Setup === #\n",
    "logger = logging.getLogger(\"data_pipeline_logger\")\n",
    "logger.setLevel(logging.DEBUG)  # Capture everything, including debug\n",
    "\n",
    "# File handler (logs everything)\n",
    "file_handler = logging.FileHandler(\"pipeline_logs.log\")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Console handler (logs info and above)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Log format\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add handlers to logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# ***Data Server Credentials WARNING \n",
    "server = 'SERVER_NAME'\n",
    "database = 'DB_NAME'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "logger.info(\"======================================================--Starting Main Script--======================================================\")\n",
    "logger.info(\"Starting DB Connection...\")\n",
    "try:\n",
    "    conn_str = f\"mssql+pyodbc://@{server}/{database}?driver={driver}&trusted_connection=yes\"\n",
    "    engine = create_engine(conn_str)\n",
    "    with engine.connect():\n",
    "        logger.info(\"DB Connection SQL Server connection successful.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"DB Connection Failed, Could not connect to SQL Server:\\n{e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Your API links\n",
    "apis = [\n",
    "    \"http://API_BASE_URL/Entity01/GetData?Param01={Param01}&Param02={Param02}&Param03=4&Param04={Param04}\",\n",
    "    \"http://API_BASE_URL/Entity02/GetData?Param01={Param01}&Param02={Param02}&Param03=4&Param04={Param04}\",\n",
    "    \"http://API_BASE_URL/Entity03/GetData?Param01={Param01}&Param02={Param02}&Param03=4&Param04={Param04}\",\n",
    "    # ,'http://API_BASE_URL/Entity03/GetAttendance?Param01={Param01}&Param02={Param02}&Param03={Param03}&Param04={Param04}&Param05={Param05}'\n",
    "]\n",
    "\n",
    "# Corresponding tables names \n",
    "table_names = [\n",
    "    'Table_01',\n",
    "    'Table_02',\n",
    "    'Table_03'\n",
    "]\n",
    "\n",
    "logger.info(\"Starting data pipeline...\")\n",
    "\n",
    "# Loop through the links and table names\n",
    "for link, table_name in zip(apis, table_names):\n",
    "    # ---==================    Getting Data From Source   ==================---\n",
    "    data = throughAPI(link)  # Call your function with the current link\n",
    "\n",
    "    # data = read_csv_to_dataframe(\"Table_01_data.csv\")\n",
    "    if not data:\n",
    "        continue\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # ---==================    Saving Data From Source   ==================---\n",
    "    bulk_load = False\n",
    "    savingData(engine, table_name, df, bulk_load)\n",
    "\n",
    "    # ---==================    Generating the data summary   ==================---\n",
    "    try:\n",
    "        data_summary(engine, table_name)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Data summaries Failed for {table_name} :\", e)\n",
    "        continue\n",
    "\n",
    "transform(engine, aggregated_table=False)\n",
    "\n",
    "for link, table_name in zip(apis, table_names):\n",
    "    # ---==================    Moving Data   ==================---\n",
    "    try:\n",
    "        move_stage2_to_stage3(engine, table_name + '_Transform_step_2', table_name)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Stage 2 to Stage 3 Failed for {table_name} :\", e)\n",
    "        continue\n",
    "\n",
    "transform(engine, aggregated_table=True)\n",
    "\n",
    "# For the Aggregated columns \n",
    "aggregated_tables = [\n",
    "    'AggTable_01',\n",
    "    'AggTable_02',\n",
    "    'AggTable_03',\n",
    "    'AggTable_04',\n",
    "    'AggTable_05',\n",
    "    'AggTable_06',\n",
    "    'AggTable_07',\n",
    "    'AggTable_08',\n",
    "    'AggTable_09',\n",
    "    'AggTable_10',\n",
    "    'AggTable_11',\n",
    "    'AggTable_12',\n",
    "    'AggTable_13',\n",
    "    'AggTable_14',\n",
    "    'AggTable_15',\n",
    "    'AggTable_16',\n",
    "    'AggTable_17',\n",
    "    'AggTable_18',\n",
    "    'AggTable_19',\n",
    "    'AggTable_20',\n",
    "    'AggTable_21',\n",
    "    'AggTable_22'\n",
    "]\n",
    "for table_name in aggregated_tables:\n",
    "    # ---==================    Moving Data   ==================---\n",
    "    move_stage2_to_stage3(engine, table_name + '_Transform_step_2', table_name, aggregated_table=True)\n",
    "\n",
    "for link, table_name in zip(apis, table_names):\n",
    "    # Run the test Cases\n",
    "    logger.info(f\"Running Test Cases for {table_name}.\")\n",
    "\n",
    "    try:\n",
    "        test_cases(engine, table_name)\n",
    "    except Exception as e:\n",
    "        logger.info(f\"One of the test cases Failed for {table_name} :\", e)\n",
    "        continue\n",
    "\n",
    "logger.info(\"Pipeline Completed...\")\n",
    "sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
